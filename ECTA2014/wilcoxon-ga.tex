\documentclass[a4paper,twoside]{article}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}        % standard LaTeX graphics tool
\usepackage{url}
\usepackage{listings}
\usepackage{subfig}
\usepackage{SCITEPRESS}
\usepackage{apalike}

\begin{document}

\title{Studying and tackling noisy fitness in evolutionary design of
  game characters}
% Antonio - lo de 'taming'...weno, pero se podría especificar meor lo del uso de 'memoria'

\author{\authorname{First Author Name\sup{1}, Second Author Name\sup{1} and Third Author Name\sup{2}}
\affiliation{\sup{1}Institute of Problem Solving, XYZ University, My Street, MyTown, MyCountry}
\affiliation{\sup{2}Department of Computing, Main University, MySecondTown, MyCountry}
\email{\{f\_author, s\_author\}@ips.xyz.edu, t\_author@dc.mu.edu}
}

% \author{J.J. Merelo\inst{1} \and Pedro A. Castillo\inst{1} \and Antonio
%   Mora\inst{1}  \and Antonio Fernández-Ares\inst{1} \and Anna I. Esparcia-Alcázar\inst{2} \and Carlos  Cotta\inst{3}}
% % Antonio - ¿No participaba Nuria también?

% \institute{University of Granada\\
%        Department of Computer Architecture and Technology, ETSIIT\\
%        18071 - Granada\\
%        \email{\{jmerelo,pedro,amorag,antares\}@geneura.ugr.es}
% \and
% S2Grupo\\
% \email{aiesparcia@s2grupo.com}
% \and
% Universidad de Málaga\\
% Departamento de Lenguajes y Sistemas Informáticos\\
% \email{ccottap@lcc.uma.es}
% }


\keywords{Evolutionary algorithms, noisy optimization problems, games,
  strategy games}

\abstract{In most computer games as in life, the outcome of a match is uncertain due
  to several reasons: the characters or assets appear in different initial
  positions or the response of the player, even if programmed, is not
  deterministic; different matches will yield different scores. That is a
  problem when optimizing a game-playing engine: its fitness will be
  noisy, and if we use an evolutionary algorithm it will have to deal
  with it. This is not straightforward since there is  an inherent
uncertainty in the true value of the fitness of an individual, or
rather whether one chromosome is better than another, thus making it preferable for selection.
% Antonio - true -> real
% No, true o en todo caso actual - JJ
% Antonio - yo creo que noisy no es saber el 'verdadero' valor del
% fitness de un individuo, eso se sabe por la función que se
% define. Otra cosa es que esa función no esté bien definida y no de
% un valor adecuado/fiable. Pero el ruido propiamente dicho consiste
% en que un mismo individuo puede tomar un valor muy bueno o muy malo
% en dos evaluaciones distintas...
% Cambiado para hablar de comparación de valores - JJ
Several methods based on implicit or explicit average or
changes in the selection of individuals for the next generation have been proposed in the past, 
% Antonio - se da por sentado que todo el mundo sabe a lo que te
% refieres con 'selection'?
% Es un congreso de GAS - jj
but they involve a substantial redesign of the algorithm and the
software used to solve the problem. In this paper we propose  new
methods based on incremental computation (memory-based) or fitness average or, additionally, using
statistical tests to impose a partial order on the population; this
partial order is considered to assign a fitness value to every individual
which can be used straightforwardly in any selection function. 
Tests using several
% Antonio - noisy/hard? Darle un poco más de bombo a los experimentos que se han hecho. Decir que es un problema ampliamente conocido, que se han hecho la tira de experimentos, etc.
% hard - JJ
hard combinatorial optimization problems show that, despite
an increased computation time with respect to the other methods, both memory-based methods have a higher
success rate than {\em implicit} averaging methods that do not use
memory; however, there is not a clear advantage in success rate or
algorithmic terms of one method over the other. 
}
\onecolumn \maketitle \normalsize \vfill
%--------------------------------------------------------------------------

\section{Introduction}

Noise in fitness has different origins. It can be inherent to the
individual that is evaluated; for instance, in 
\cite{DBLP:journals/jcst/MoraFGGF12} a game-playing bot (autonomous agent) that includes a set of application rates is optimized. This results in different
actions in different runs, and obviously different success rates and
then fitness. Even comparisons with other individuals can be affected:
given exactly the same pair of individuals, the chance of one beating
the other can vary in a wide range. In other cases like the one
presented in the MADE environment, where whole worlds are evolved
\cite{2014arXiv1403.3084G} the same kind of noisy environment will
happen.  When using evolutionary algorithms to optimize stochastic
methods such as neural networks \cite{castilloGECCO99}
using evolutionary algorithms the measure that is usually taken as
fitness, the success rate, will also be noisy since different training
schedules will result in slightly different success rates. 

The examples mentioned above are included actually in one or the four categories
where uncertainties in fitness are found, fitness functions with intrinsic noise. These four types include also,
% Antonio - ¿cómo se llama la categoría que has contado arriba?
% olvidado y añadido - JJ 
according to \cite{Jin2005303} approximated fitness functions
(originated by, for instance, surrogate models); robust functions,
where the main focus is in finding values with high tolerance to
change in initial evaluation conditions, and finally dynamic fitness
functions, where the {\em inherent} value of the function changes with
time. 
% Antonio - yo desarrollaría un poco más los diferentes tipos y en qué tipo de problemas se encuentran, por ejemplo, ya que la intro queda un poco escueta
% Es que no cabe más. 
Our main interest will be in the first type, since it is the one
that we have actually met in the past and which has led to the development
of this work. 

At any rate, in this paper we will not be dealing with actual
problems; we will try to simulate the effect of noise by adding to the fitness function gaussian noise centered in 0 and $\sigma=1,2,4$. We will deal mainly with combinatorial optimization functions with noise added having the same shape and amplitude,
that we actually have found in problems so far. In fact, from the
point of view of dealing with fitness, these are the main features of
noise we will be interested in. 

% Antonio - explicar qué se hará en el artículo para simular estos fitness, el Wilcoxon tournament, o al menos describir los experimentos
% explicado algo - JJ

The rest of the paper is organized as follows: next we describe the
state of the art in the treatment of noise in fitness functions. The
method we propose in this paper, called Wilcoxon Tournament, will be
shown in Section \ref{sec:wilcoxon}; experiments are described and
results shown in Section \ref{sec:res}; finally its implications are discussed in the last section of the paper. 

%--------------------------------------------------------------------------

\section{State of the art}
\label{sec:soa}

The most comprehensive review of the state of the art in evolutionary
algorithms in {\em uncertain} environments 
was done by Jin and Branke in 2005 \cite{Jin2005303}, although recent papers such as \cite{DBLP:journals/corr/QianYZ13} include a brief update of the state
of the art. In that first survey 
of evolutionary optimization by Jin and Branke  in
uncertain environments this uncertainty is categorized into noise,
robustness, fitness approximation and time-varying fitness functions, and then,
different options for dealing with it are proposed. In principle, the
approach presented in this paper was designed to deal with the first kind of
uncertainty, noise in fitness evaluation, although it could be argued
that there is uncertainty in the true fitness as in the third
category. In any case it could be applied to
other types of noise. In this situation, several solutions were been
proposed and explained in the survey \cite{Jin2005303}. These will be
explained next.

An usual approach is just disregarding the fact that the fitness is
noisy and using whatever value is returned each time or after
re-evaluation each generation. 
This is the usual approach in % our
previous research \cite{castilloGECCO99,bots:evostar} and leads, if
the population is large enough, to an {\em implicit averaging} as
mentioned in \cite{Jin2005303}. In fact, evolutionary algorithm
selection is also stochastic, so noise in fitness evaluation
will have the same effect as randomness in selection or a higher mutation
rate, which might make the evolution process easier and not harder
in some particular cases
\cite{DBLP:journals/corr/QianYZ13}. 
In fact, Miller and Goldberg proved that an infinite population would not
be affected by noise \cite{miller1996genetic} and Jun-Hua and Ming studied the
effect of noise in convergence rates \cite{Junhua20136780}, proving
that an elitist genetic algorithm finds at least one solution with a lowered
convergence rate. But populations are finite, so the usual approach is to increase the population size to a value bigger than would be needed in a non-noisy environment. 
This has also the advantage that no special provision or change to the
implementation has to be made; but a different value of a single parameter.

Another more theoretically sound way is using a statistical central tendency
indicator, which is usually the {\em average}. This strategy is called
{\em explicit averaging} by Jin and Branke and is used, for instance,
in 
\cite{Junhua20136780}. Averaging decreases the variance of fitness but
the problem is that it is not clear in advance what would be the
sample size used for averaging \cite{aizawa1994scheduling}. Most
authors use several measures of fitness for each new individual
\cite{costa2013using}, although other averaging strategies have also
been proposed, like averaging over the neighbourhood of the
individual or using resampling, that is, more measures of fitness in a
number which is decided heuristically \cite{liu2014mathematically}. This assumes that there is, effectively, an average of the
fitness values which is true for Gaussian random noise and other
distributions such as Gamma or Cauchy but not
necessarily for all distributions. 
To the best of our knowledge, 
other measures like the median which might be more adequate for
certain noise models have not been tested; the median always exists, while the average might not exist for non-centrally distributed variables. Besides, most models keep the number of evaluations is fixed
and independent of its value, which might result in bad individuals
being evaluated many times before being discarded; some authors have
proposed {\em resampling}, that is, re-evaluate the individuals a number of times to increase the precision in fitness
\cite{RadaVilela2014}, 
which will effectively increase the number of
evaluations and thus slow down the search. In any case, using average is
also a small change to the overall algorithm framework, requiring only
using as new fitness function the average of several evaluations.
We will try to address this in the model presented in this
paper. 

These two approaches that are focused on the evaluation process might
be complemented with changes to the selection process. For instance,
using a threshold \cite{Rudolph2001318} that is related to the noise characteristics to
avoid making comparisons of individuals that might, in fact, be very
similar or statistically the same; this is usually called {\em
  threshold selection} and can be applied either to explicit or
implicit averaging fitness functions. The algorithms used for
solution, themselves, can be also tested, with some authors proposing, instead of taking more measures, 
testing different solvers \cite{cauwet2014algorithm}, some of which
might be more affected by noise than others. 

Any of these approaches do have the problem of statistical
representation of the {\em true} fitness, even more so if there is not
such a thing, but several measures that represent, {\em all of them}
the fitness of an individual. This is what we are going to use in this
paper, where we present a method that uses resampling via an
individual memory and use either explicit averaging or statistical
tests like the non-parametric Wilcoxon test. 
First we will examine and try to find the shape of the noise that
actually appears in games; then we will check in this paper what is the influence on the quality of
results of these two strategies and which one, if any, is the best when working in noisy environments. 

%--------------------------------------------------------------------------

\section{Noise in games: An analysis}
\label{sec:noise}

In order to measure the nature of noise, we are going to use the
Planet Wars game, that has been used as a framework for evolving
strategies, for instance, in \cite{DBLP:journals/jcst/MoraFGGF12}. In
this game, initial position of the players is random with the
constraint that they should be far enough from each other; other than
that, any planet in the game can be an initial position. Besides, in
the strategy used in that game, actions are not deterministic, since
every player is defined by a set of probabilities to take one course
of action.

An evolutionary algorithm with standard parameters was run with the
main objective of measuring the behavior of fitness. A sample of ten
individuals from generation 1, and another 10 from generation 50 were
extracted. The fitness of each individual was measured 100 times. 
The main intention was also to see how noise evolved with
time. Intuitively we thought that, since the players become better
with evolution, the noise and thus the standard deviation would
decrease. However, what we found is plotted in Figure \ref{fig:sd},
which shows a plot of the standard deviation in both generations and
illustrates the fact that the spread of fitness values is {\em bigger}
as the evolution proceeds, going from around 0.15 to around 0.20. 
%
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{sd.png}
\caption{Boxplot of standard deviation of noise fitted to a normal
  distribution, left for the first generation and right for the
  50th. Fitness averages around 1. \label{fig:sd}}
\end{figure}

This might, in fact, be a bit misleading since the average values of
the fitness increase at the same time, but it does mean that the noise
level might be around 20\% of the {\em signal} in problems such as
this one. 

But we were also interested in checking whether, in fact, the normal
distribution is the best fit for the fitness measures. We tested three
distributions: Gamma, Weibull and normal (Gaussian) distribution after
doing an initial test that included Cauchy and Exponential. All this
analysis was done using the library {\tt fitrdist} in R, and data as
well as scripts needed to do it are publicly available. After trying
to fit these three distribution to data in generation 1 and 50, we
analyzed goodness-of-fit using the same package and the {\tt gofstat}
function. This function yields several measures of goodness, including
the Akaike Information Criterion and the Kolmogorov-Smirnov statistic.

What we found was that, in all cases, Gamma is the distribution that
better fits the data. That does not mean that the noise effectively
follows this distribution, but that if it can be said to follow one,
it's this one. In fact, just a few individuals have a good fit (to
95\% accuracy using the Kolmogorov-Smirnov), and none of them in
generation 50. The fit for an individual in the first generation that
does follow that distribution (individual 8) is shown in figure
\ref{fig:indi8}. 
%
\begin{figure*}
\centering
\includegraphics[scale=0.8]{indi8.png}
\caption{Fit of the fitness value of an individual in the first
  generation of the evolutionary algorithm to a gamma distribution,
  showing an histogram in the top left corner, CDFs in the bottom left
  corner and quantile-quantile and percentile-percentile plots in the
  right hand side. \label{fig:indi8}}
\end{figure*}

This figure also shows that, even if it is skewed, its skewness is not
too high which makes it close to the standard distribution (which is
considered a good
approximation if $k$ > 10). 

Some interesting facts that can be deduced from these measures is that
in general, fitness is skewed and has a high value. Besides, it
follows a gamma distribution, which, if we wan to model noise
accurately, should be the one used. However, we are rather interested
in the overall shape of noise so since the skewness value of the gamma
distribution is rather high we will use, in this paper, a gaussian
noise to simulate it. This will be used in the experiments shown
below. 


\section{Fitness memory and statistical significant differences}
\label{sec:wilcoxon}

As indicated in previous section, most explicit averaging methods
use several measures to compute fitness as an average, with
resampling, that is, additional measures, in case 
% Antonio - that comparisons?
comparisons are not
statistically significant. In this paper we will introduce a fitness
{\em memory}, which amounts 
to a resampling every generation an
individual survives. An individual is born with a fitness
memory of a single value, with memory size increasing with {\em
  survival} time. This is actually a combination of an implicit and an 
explicit evaluation strategy: {\em younger} individuals are rejected
outright if their fitness computed after a single evaluation is not
enough to participate in the pool, while  {\em older} ones use several
measures to compute average fitness, which means that averages will be
a more precise representation of actual value. 
% Antonio - actual -> real/descriptive?
% Antonio, por favor. "Actual" en inglés significa "real" - JJ
As evolution proceeds,
the best individuals will, effectively, have an underlying non-noisy
best value. We will call this method {\em incremental temporal average}
or {\ sf ITA}. 
% Antonio - tampoco me mola lo de ITA, mejor 'incremental' o
% 'improving'
% Cambiado - JJ

However, since average is a single value, selection methods might, in
fact, select as better individuals 
% Antonio - as best?
% better. comparando entre dos - JJ
some that are not if the comparison
is not statistically significant; this will happen mainly in the first
and middle stages of search, which might effectively eliminate from
the pool or not adequately represent individuals that constitute, in
fact, good solutions. That is why we introduce an additional feature:
using Wilcoxon test \cite{wilcoxon:1945}
% Antonio - Wilcoxon test? Poner cita y/o breve explicación del test...
% añadida. - JJ
for comparing not the average, but all
fitness values attached to an individual. This second method
introduces a partial order in the population pool: two individuals
might be different (one better than the other) or not. There are many
possible ways of introducing this partial order in the evolutionary
algorithm; however, what we have done is to pair individuals a certain
number of times (10, by default) and have every individual score a
point every time it is better than the other in the couple; it will
get a point less if it is the worse one. An individual that is better
that all its couples will have a fitness of 20; one whose comparisons
are never significant according to the Wilcoxon test will score
exactly 10, the same as if it wins as many times as it loses, and the
one that always loses will score 0. We will call this method
Wilcoxon-test based partial order, or {\sf WPO} for short.
% Antonio - cursiva. ;)
% san serif - JJ

\begin{figure*}
\centering
\subfloat[Memory size]{
\includegraphics[width=0.45\textwidth]{memory-size.png}
}
~
\subfloat[Fitness distribution]{
\includegraphics[width=0.45\textwidth]{fitness-distribution.png}
}
\caption{(Left) 3D plot of the distribution of memory sizes for a single
  execution of the Wilcoxon-test based partial order. (Right)
  3d plot of the distribution of fitness values along time for the WPO
  method on the Trap function. \label{fig:initial}}
\end{figure*}
% Antonio - por qué están las figuras 'torcidas'? La Z normalmente está vertical
% No he visto mejor forma de sacarlas - JJ
Initial tests were made with these two types of algorithms and the
Trap function \cite{wilcoxon:ga}, showing the best results for the WPO method  and both of them being better, on that problem, that
the implicit average method that uses a single noisy evaluation per individual. 
% Antonio - la frase anterior no parece bien escrita
% sí lo está, pero he aclarado algo por si acaso  JJ 
The only aspect in which this method is better is in speed and memory
footprint. Since it does not need to perform averages or make
additional fitness measures every generation, it is twice as fast as
the next method, the one that uses explicit average fitness. An
exploration of memory sizes (shown in Figure \ref{fig:initial}, left; a rotating version of the graph is published in
  \url{http://jj.github.io/Algorithm-Evolutionary/graphs/memory/}) for a
typical run) showed that there is an uneven distribution of memory
sizes and thus ages but, in general, there is no single memory size
overcoming all the population. Besides, distribution of fitness, shown
for a typical run in Figure \ref{fig:initial} (right, also published at
  \url{http://jj.github.io/Algorithm-Evolutionary/graphs/fitness-histo/}) shows a
distribution with most values concentrated along the middle (that is,
fitness equal to 10 or individuals that cannot be compared with any
other, together with a few with the highest fitness and many with the
lowest fitness. Besides showing that using the partial order for individual selection 
% Antonio - partial order respecto a qué?
% respecto a nada. Es una relación entre individuos Aclarado - JJ
is a valid strategy, it also shows that a too greedy selection method would
eliminate many individuals that might, in fact, have a high fitness if
only given the chance to be evaluated for another generation. This
will be taken into account when assigning parameter values to the
evolutionary algorithm that will be presented next.

All these initial tests have been programmed using {\tt
  Algorithm::Evolutionary}
% Antonio - Evolu-i-tionary? Supongo que no, pero por si le hicite algún cambio y lo renombraste. :D
% corregido - JJ
 (in Perl) \cite{ae09} and code and data
% Antonio - esa cita no está en el bib
% Está en geneura.bib - JJ
are available under an open source licence from the library
repository at GitHub
\url{https://github.com/JJ/Algorithm-Evolutionary/}. However, it was
clear from these tests that the type of problems in which every method
works the best needs to be characterized, so we will drill further
into this with another set of experiments that will be presented
next. 

%--------------------------------------------------------------------------

\section{Results}
\label{sec:res}
%
\begin{table*}[!htbp]
\begin{center}
\caption{Common evolutionary algorithm parameters}
\label{fig:ga_params}
\begin{tabular}{lc}%{p{3cm}p{7cm}}
\hline\noalign{\smallskip}
\noalign{\smallskip}
Parameter & Value \\
\hline
\noalign{\smallskip}
Chromosome length & 40 (Trap) 60 (MMDP)\\
Population size & 1024\\
Selection & 2 tournament selection \\
Replacement rate & 50\% \\
Mutation rate & 20\% \\
Crossover rate &  80\% \\
Max evaluations & 200K (Trap) 1 Million(MMDP) \\
Stopping criterion & Non-noisy best found or max evaluations reached \\
\hline
\end{tabular}
\end{center}
\end{table*}
%
ITA and WPO have been tested using two well-known benchmarks, the deceptive bimodal Trap \cite{deb1992analyzing}
function and the Massively Multimodal Deceptive Problem \cite{goldberg92massive} (MMDP). 
% Antonio - Acabas de nombrar al MMDP por primera vez en el artículo. Parecía que sólo ibas a usar Trap. Además habría que medio explicarlo o poner una cita. ;)
% puestas dos - JJ
We chose to use just these two functions were chosen
because they have different fitness landscapes, are usually difficult
for an evolutionary algorithm and have been extensively used for
testing other kind of operators and algorithms.
% Antonio - Qué pasó con el fitness de Antares?
% Me dio la forma de la función, que era aproximadamente gaussiana - JJ
 Besides, they are part
of the {\tt Algorithm::Evolutionary} standard set of fitness
functions \cite{ae:gh}. 

Several methods were tested: a baseline algorithm without
noise, that gave us an idea on the time and number of evaluations
needed to find the solution, a 0-memory (implicit average) method that
uses noisy fitness without making any special arrangement, ITA and
WPO. Evolutionary algorithm parameters (listed in Table \ref{fig:ga_params})  and code for all tests were the same (except in one
particular case. We have also used an additive gaussian noise centered in zero
and different $\sigma$, which is independent of the fitness values
range. By default, noise will follow a normal distribution with center in 0 and $\sigma=1$.
% Antonio - explicar un poco esto del ruido centrado en sigma, que es la parte fundamental del trabajo (de los experimentos) y muchos lectores (como yo :D) no lo sabrán.
% Es ruido gaussiano. Explicado. - JJ

\begin{figure*}[htbp]
\centering
\subfloat[Memory size]{
\includegraphics[width=0.5\textwidth]{trap-evals-all.png}
}
~
\subfloat[Fitness distribution]{
\includegraphics[width=0.5\textwidth]{trap-noisy-evals.png}
}
\caption{(Left) Comparison of number of evaluations for the 4-Trap x
  10 function and the rest of the algorithms with a noise $\sigma$
  equal to 1. (Right). Plot of average number of evaluations for
  different methods: 0 memory (black, solid), ITA (red, dashed), WPO
  (blue, dot-dashed). \label{fig:trap:evals}}
\end{figure*}
%
All tests use the {\tt Algorithm::Evolutionary} library, and the scripts
are published, as above, in the GitHub repository, together with raw
and processed results. The evolutionary algorithm code used in all
cases is exactly the same except for WPO, which, since it needs the
whole population to evaluate fitness, needed a special reproduction
and replacement library. This also means that the replacement method
is not exactly the same: while WPO replaces every generation 50\% of
the individuals, the rest evaluate new individuals before replacement
and eliminate the worst 512 (50\% of the original
population). Replacement is, thus, less greedy in the WPO case, but
we do not think this will be a big influence on result (although it
might account for the bigger number of evaluations obtained in some
cases), besides, it just needed a small modification of code and was
thus preferred for that reason. All values shown are the result of 30
independent runs.

The results for different noise levels are shown in Figure
\ref{fig:trap:evals}. The boxplot on the left hand side compares the
number of evaluations for the baseline method and the three methods
with  $\sigma=1$. The implicit average method (labelled as 0-memory) is
only slightly worse than the baseline value of around 12K evaluations,
with the ITA and WPO methods yielding very similar values which are
actually worse than the 0-memory method. However, the scenario on the
right,
 which shows how the number of evaluations scales with the noise
level, is somewhat different. While the 0-memory method still has the
least number of evaluations {\em for successful runs}, the success
rate degrades very fast, with roughly the same and slightly less than
100\% for $\sigma=2$ but falling down to 63\% for 0-memory and around
80\% for ITA and WPO (86\% and 80\%). That is, best success rate is
shown by the ITA method, but the best number of evaluations for
roughly the same method is achieved by WPO. 

These results also show that performance degrades quickly with problem
difficulty and the degree of noise, that is why we discarded the
0-memory method due to its high degree of failure (a high percentage of the runs did not find the solution) 
% Antonio - failure en qué sentido?
% pues que no encuentran la solución - JJ 
with noise = 10\%
max fitness and evaluated ITA and WPO over another problem, MMDP with
similar absolute $\sigma$, with the difference that, in this case,
$\sigma=2$ would be 20\% of the max value, which is close to the one
observed experimentally, as explained in the Section \ref{sec:noise}.

\begin{figure*}[!htbp]
\centering
\includegraphics[width=0.6\textwidth]{evals-mmdp.png}
\caption{Number of evaluations for successful runs ITA and WPO needed for solving the MMDP
  problem with 6 blocks and different noise levels, $\sigma=1,2$. \label{fig:mmdp:evals}}
\end{figure*}
%
The evolutionary algorithm for MMDP used exactly the same parameters
as for the Trap function above, except 
% Antonio - execpt for?
% no - JJ
the max number of evaluations
was boosted to one million. Initial tests with the 0-memory method
yielded a very low degree of success, which left only the two methods
analyzed in this paper for testing with MMDP. 
% Antonio - esta frase (arriba) no la entiendo bien, parece incompleta...
% terminada - JJ
Success level was in all cases around 90\% and very similar in all experiments; the number of evaluations is more affected by noise and shown in
Figure \ref{fig:mmdp:evals}. 
% Antonio - ese 'but' no parece correcto aquí
% correcto - JJ
In fact, WPO and ITA have a very similar
number of evaluations. It is statistically indistinguishable for
$\sigma=2$, and different only at the 10\% level (p-value = 0.09668)
for $\sigma=1$, however, if we take the time needed to reach solutions
into account, ITA is much faster since it does not apply 10*1024
statistical tests every generation. However, WPO is more robust, with
a lower standard deviation, in general, at least for high noise
levels. 

At any rate, both methods obtain a good result with a much higher
success rate than the implicit fitness (0-memory) method. Besides, ITA and APO 
% Antonio - a qué se refiere el 'it'?
% a los dos métodos, corregido - JJ
incorporate explicit fitness evaluation naturally into the population
{\em resampling} only surviving individuals. This accounts for a
predictable behaviour of the algorithm, since the number of evaluations
per generation is exactly the population size, which is important for
optimization processes with a limited budget. 

%--------------------------------------------------------------------------

\section{Conclusions}

In this paper we have introduced two methods to deal with the problem
of noisy fitness functions. The two methods, {\sf ITA}, based on
re-evaluation of surviving individuals and {\sf WPO}, which uses the
Wilcoxon test to compare a sample of individuals and partial-order
them within the population, 
 have been tested over two
different fitness functions and compared with implicit average (or
0-memory) methods, as well as among themselves. In general,
memory-based methods have much higher success rate than 0-memory
methods and the difference increases with the noise level, with
0-memory methods crashing at noise levels close to 20\% while ITA and
WPO maintain a high success level.

It is difficult to choose between the two proposed methods, ITA and
WPO. However, ITA is much faster since it avoids costly
comparisons. It also has a slightly higher success rate, and the
number of evaluations it needs to find the solution is only slightly
worse; even if from the point of view of the evolutionary algorithm it
is slightly less robust and slightly worse, it compensates the time
needed to make more evaluations with the fact that it does not need to
perform statistical tests to select new individuals.

However, this research is in initial stages. The fact that we are
using a centrally distributed noise gives ITA an advantage since, in
fact, comparing the mean of two individuals will be essentially the
same as doing a statistical comparison, since when the number of
measures is enough, statistical significance will be reached. In fact,
with a small difference ITA might select as better an individual whose
fitness is actually the same, something that would be correctly
spotted by WPO, but, in fact, since there is an average selective
pressure this is not going to matter in the long run. 

It might matter in different situations, for instance in numerical
optimization problems and also when noise follows an uniform
distribution; behavior might in this case be similar to when noise
levels are higher. These are scenarios that are left for future
research, and destined to find out in which situations WPO is better
than ITA and the other way round. 

Finally, there are other free parameters that will have to be
explored. The first one is the number of comparisons in WPO. Initial
explorations have proved that changing it from 5 to 32 does not yield
a significant difference. Looking for a way to speed up this method
would also be important since it would make its performance closer to
ITA. Memory size could also be explored. Right now evaluations are
always performed, but in fact after a number of evaluations are done
comparisons will be statistically significant; it is difficult to
know, however, which is this number, but in long runs it would be
interesting to cap fitness memory size to a sensible number, or, in
any case, see the effect of doing it. 

From the point of view of the noise, we will have to examine the
nature of the noisy fitness in many different experiments so that we
can model more accurately the way it works and propose changes to the
evolutionary algorithm to deal with them. This evolutionary algorithm
will also have to take into account a more accurate Gamma
distribution, instead of the normal distribution used in this paper,
as well as the noise levels found experimentally.


\section{Acknowledgments}

% {\small This work has been supported in part by project ANYSELF
% (TIN2011-28627-C04-02 and -01).
% The authors would like to thank the FEDER of European Union for
% financial support via project "Sistema de Información y Predicción de
% bajo coste y autónomo para conocer el Estado de las Carreteras en
% tiempo real mediante dispositivos distribuidos" (SIPEsCa) of the
% "Programa Operativo FEDER de Andalucía 2007-2013". We also thank all
% Agency of Public Works of Andalusia Regional Government staff and
% researchers for their dedication and professionalism.}

% \begin{figure*}
% \begin{center}
% \includegraphics[width=3cm]{logos_SIPESCA_2.jpg}
% \end{center}
% \end{figure*}

\bibliographystyle{apalike}
\bibliography{geneura,wilcoxon}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
