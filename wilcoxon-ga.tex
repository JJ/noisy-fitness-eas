\documentclass{llncs}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}        % standard LaTeX graphics tool
\usepackage{url}
\usepackage{listings}

\begin{document}

\title{Taming noisy fitness using memory and Wilcoxon statistical test}
\subtitle{}

\author{J.J. Merelo\inst{1} \and Pedro A. Castillo\inst{1} \and Antonio
  Mora\inst{1}  \and Anna I. Esparcia-Alcázar\inst{2} \and Carlos  Cotta\inst{3}}

\institute{University of Granada\\
       Department of Computer Architecture and Technology, ETSIIT\\
       18071 - Granada\\
       \email{\{jmerelo,pedro,amorag\}@geneura.ugr.es}
\and
S2Grupo\\
\email{aiesparcia@s2grupo.com}
\and
Universidad de Málaga\\
Departamento de Lenguajes y Sistemas Informáticos\\
\email{ccottap@lcc.uma.es}
}

\maketitle
\begin{abstract}
Noisy evaluation functions show up in may different optmization
problems, from industrial optimization to strategy games. Dealing with
them is not straightforward because of the inherent uncertainty in the
true value of the fitness of an individual, if it actually
exists. Several methods based on implicit or explicit average or
changes in selection have been proposed in the past, but they involve
a substantial redesign of the algorithm and the software used to solve
the problem. In this paper we propose a new method based on using
statistical tests to impose a partial order on the population; this
partial order is used to assign a fitness value to every individual
which can be used straitghforwarly in any selection function. 
Tests over a combinatorial optimization problem show that, despite
increasing computation time, the number of evaluations needed to reach
a solution is much smaller than the one needed by other methods that
use implicit or explicit averages for the fitness function. 
\end{abstract}

\section{Introduction}

Noise in fitness has different origins. It can be inherent to the
individual that is evaluated; for instance, in 
\cite{DBLP:journals/jcst/MoraFGGF12} a game-playing bot that includes a
set of application rates is optimized. This results in different
actions in different runs, and obviously different success rates and
then fitness. Even comparisons with other individuals can be affected:
given exactly the same pair of individuals, the chance of one beating
the other can vary in a wide range. In other cases like the one
presented in the MADE environment, where whole worlds are evolved
\cite{2014arXiv1403.3084G} the same kind of noisy environment will
happen;  when using evolutionary algorithms to optimize  stochastic
methods such as neural networks \cite{castilloGECCO99}
using evolutionary algorithms the measure that is usually taken as
fitness, the success rate, will also be noisy since different training
schedules will result in slightly different success rates. 

The examples mentioned above are actually one or the four categories
where uncertainties in fitness are found. These four types include also,
according to \cite{Jin2005303} approximated fitness functions
(originated by, for instance, surrogate models); robust functions,
where the main focus is in finding values with high tolerance to
change in initial evaluation conditions, and finally dynamic fitness
functions, where the {\em inherent} value of the function changes with
time, Our main interest will be in the first type, since it is the one
that we have actually met in the past and has led to the development
of this paper. 

At any rate, in this paper we will not be dealing with actual
problems; we will try to simulate the effect of noise on combinatorial
optimization functions using the same shape, and hopefully, amplitude,
that we actually have found in problems so far. In fact, from the
point of view of dealing with fitness, these are the main features of
noise we will be interested in. Besides, we will deal mainly with
additive noise with 0 mean and variance equal to 1.

The rest of the paper is organized as follows: next we describe the
state of the art in the treatment of noise in fitness functions. The
method we propose in this paper,  called Wilcoxon Tournament, will be
shown in Section \ref{sec:wilcoxon}; experiments are described and
results shown in Section \ref{sec:res} and its implications
discussed in the last section of the paper. 

\section{State of the art}
\label{sec:soa}

The best review of the state of the art was done by Jin and Branke in
2005 \cite{Jin2005303}, although recent papers such as
\cite{DBLP:journals/corr/QianYZ13} include a brief update of the state
of the art. In this survey of evolutionary optimization in
uncertain environments this uncertainty is categorized and then
different options for dealing with it are proposed. In principle, the
approach presented in this paper was designed to deal with the first kind of
uncertainlty, noise in fitness evaluation, but it could be applied to
other types of noise. In this situation, several solutions have been
proposed and explicited in the survey.

An usual approach is just disregarding the fact that the fitness is
noisy and using whatever value is returned a single time or after
re-evaluation each generation. This is the usual approach in our
previous research \cite{castilloGECCO99,bots:evostar} and leads, if
the population is large enough, to an {\em implicit averaging} as
mentioned in \cite{Jin2005303}. In fact, evolutionary algorithm
selection is also stochastic and noise in fitness evaluation
will have the same effect as noise in selection or a higher mutation
rate and eventually make the evolution process easier and not harder
in some particular cases
\cite{DBLP:journals/corr/QianYZ13}. In fact, Miller and Goldberg proved that an infinite population would not
be affedted by noise \cite{miller1996genetic} and Jun-Hua studied the
effect of noise in convergence rates \cite{Junhua20136780} proving
that an elitist GA finds at least one solution with a lowered
convergence rate. But finite populations
are, so the usual approach is to increase population size to a value
bigger than would be needed in a non-noisy environment. This has also
the advantage that no special provision or change to the
implementation has to be made; it is simply a matter of changing the
value of a single parameter.

Another more theoretically sound way is  using a statistical central tendency
indicator, which is usually the {\em average}. This strategy is called
{\em explicit averaging} by Jin and Branke
\cite{Junhua20136780}. Averaging decreases the variance of fitness but
the problem is that it is not clear in advance what would be the
sample size used for averaging \cite{aizawa1994scheduling} but most
authors use several measures of fitness for each new individual
\cite{costa2013using}, although other averaging srategies have also
been proposed, like averaging over the neighborhood of the
individual. This assumes that there is, effectively, an average of the
fitness values which is true for gaussian random noise but not
necessarily for other distributions. Jin and Branke do not mention
(and we have not been able to find, although we are sure somebody must
have tested it)
other measures like the median which might be more adequate for
certain noise models. Besides, in many cases the number of evaluations is kept fixed
and independent of its value, which might result in bad individuals
being evaluated many times before being discarded; some authors have
proposed {\em resampling}, that is, re-evaluate the number of
individuals to increase the precision in fitness
\cite{RadaVilela2014}, which will effectively increase the number of
evaluations and thus slow down search. In any case, using average is
also a small change to the overall algorithm framework, requiring only
using as new fitness function the average of several evaluations.
We will try to address this in the model presented in this paper.

These two approaches that are focused on the evaluation process might
be complemented with changes to the selection process. For instance,
using a threshold \cite{Rudolph2001318} that is related to the noise characteristics to
avoid making comparisons of individuals that might, in fact, be very
similar or statistically the same; this is usually called {\em
  threshold selection} and can be applied either to explicit or
implicit averaging fitness functions. 

All these approaches have a problem: average might not, in fact,
exist, and even if it does, using averages to compare might not be
statistically significant; if resampling is used to achieve
statistical significance more evaluations than needed might have to be
done. What we will do in this paper is to use resampling via an
individual memory and use either explicit averaging or statistical
comparisons. We will check in this paper what is the influence on
search of these two strategies. 


\section{Fitness memory and statistical significant differences}
\label{sec:wilcoxon}


\section{Results}
\label{sec:res}

\begin{table}[!t]
\centering
\begin{tabular}{|l|r|r|}
\hline 
Method & Number of evaluations & Time\\
\hline
Baseline & 20752 $\pm$ 2528 & 1.2907 $\pm$ 0.16\\
\hline
0-Memory & 190855   $\pm$ 22370 & {\bf 2.64 $\pm$ 0.31}\\
Memory + Avg & 276100 $\pm$  55088 & 22.88 $\pm$ 6.46\\
WPO & {\bf 56081 $\pm$ 11525} & 82.19 $\pm$ 23.46\\
\hline
\end{tabular}
\vspace{1em}
\caption{Comparison between the baseline 4-Trap results and those
  obtained by the rest of the methods; WPO or Wilcoxon-test based
  partial ordering algorithm is the one introduced in this paper.\label{tab:res}}
\end{table}

\section{Conclusions}



\section{Acknowledgments}

This work has been supported in part by project ANYSELF
(TIN2011-28627-C04-02 and -01).
The authors would like to thank the FEDER of European Union for
financial support via project "Sistema de Información y Predicción de
bajo coste y autónomo para conocer el Estado de las Carreteras en
tiempo real mediante dispositivos distribuidos" (SIPEsCa) of the
"Programa Operativo FEDER de Andalucía 2007-2013". We also thank all
Agency of Public Works of Andalusia Regional Government staff and
researchers for their dedication and professionalism.

\begin{figure}
\begin{center}
\includegraphics[width=6cm]{logos_SIPESCA_2.jpg}
\end{center}
\end{figure}

\bibliographystyle{splncs03}
\bibliography{geneura,wilcoxon}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
